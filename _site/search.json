[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am an R/Shiny Developer with expertise in helping nonprofits collect, manage and analyze their program data. I learned data science techniques to make these jobs better.\n Pronouns: he/him\n I‚Äôm currently working on improving my data pipelines, and communicating data with Shiny Applications.\n I‚Äôm currently learning Bash and SQL\n I‚Äôm looking to collaborate on Shiny Applications.\n Ask me about data in the nonprofit sector.\n How to reach me: kevin.gilds@hey.com\n\n\n\n   \n\n\n\n\n\n\n\nShiny Posts\nR Posts\n\n\n\n\n\nClick to Expand\n\nIntro to Python | 2021-08-09 |¬†Certificate\nIntro to Machine Learning | 2021-08-10 | Certificate\nIntermediate Machine Learning | 2021-08-14 | Certificate\nFeature Engineering | 2021-09-18 | Certificate\nPandas | 2021-09-23 | Certificate\n\n\n\n\n\n\n\nmy chromebook\n\n\n\n\nClick to Expand\n\nIntroduction to Chromebook Data Science\nHow to use a Chromebook\nGoogle and the Cloud\nOrganizing Data Science Projects\nVersion Control\nData Tidying\nData Visualization\nGetting Data"
  },
  {
    "objectID": "posts/2023-02-21-email-on-edit/index.html",
    "href": "posts/2023-02-21-email-on-edit/index.html",
    "title": "Email on edit",
    "section": "",
    "text": "I have been using Apps Script at work a lot and this video is what I have been looking for."
  },
  {
    "objectID": "posts/2023-02-20-checklists/index.html",
    "href": "posts/2023-02-20-checklists/index.html",
    "title": "Checklists",
    "section": "",
    "text": "Check this websiteout for more about the meta of checklists"
  },
  {
    "objectID": "posts/2022-11-23-app-script/index.html",
    "href": "posts/2022-11-23-app-script/index.html",
    "title": "App Script",
    "section": "",
    "text": "I have been thinking about automation in the Google Workspace. There are a lot of cool things one can do with javascript with google sheets and forms.\nThis morning, I worked through a tutorial on the submit of a form to use the data to update a form template and send an email.\nThe person who posts video to this channel is a good teacher\nhttps://www.youtube.com/@ExcelGoogleSheets"
  },
  {
    "objectID": "posts/2021-11-01-issue-templates/index.html",
    "href": "posts/2021-11-01-issue-templates/index.html",
    "title": "Issue-Templates",
    "section": "",
    "text": "Yes, I really needed an issue template and recalled this tweet\n\n\nBeen revamping some SOPs :: code review checklists. Learned about GitHub Issue templates which are awesome for this. Here‚Äôs an #rstats specific one https://t.co/VN0mkmHjQ7 pic.twitter.com/FvEpgHRCPL\n\n‚Äî Travis Gerke (@travisgerke) September 29, 2021\n\n\nHowever, for this project I started using gitlab as I wanted to try the updated gitlabr package. I found this issue-templates. The directions are okay, but I found that I had to use the web ‚Äòide‚Äô to create the directory in the gitlab directory.\nAnd like I figured, issue templates have been super helpful."
  },
  {
    "objectID": "posts/2021-12-08-linux-twitter/index.html",
    "href": "posts/2021-12-08-linux-twitter/index.html",
    "title": "Linux-Twitter",
    "section": "",
    "text": "Linux shell script to reduce PDF file sizeHere is a handy and useful Linux and Unix shell script that reduce PDF file size using Ghostscript. No need to upload your PDF file to the shady third-party website.https://t.co/h1l60vHwOb\n\n‚Äî howtopam (@howtopam) December 6, 2021"
  },
  {
    "objectID": "posts/joins/joins.html",
    "href": "posts/joins/joins.html",
    "title": "joins",
    "section": "",
    "text": "SELECT * FROM <TABLE 1> JOIN TYPE <TABLE 2> ON <TABLE 1.KEY> <TABLE 2.KEY>\nIf the two keys are the same, one can use the Using function.\n{eval= FALSE} SELECT * FROM <table_1> FULL JOIN <table_2> USING(<column>);"
  },
  {
    "objectID": "posts/2023-04-28-data-dictionary/index.html",
    "href": "posts/2023-04-28-data-dictionary/index.html",
    "title": "Data Dictionary",
    "section": "",
    "text": "Here is a good article about creating data dictionaries using the pointblank package"
  },
  {
    "objectID": "posts/2023-04-03-what-to-write/index.html",
    "href": "posts/2023-04-03-what-to-write/index.html",
    "title": "What to write",
    "section": "",
    "text": "Thus learning JavaScript and html."
  },
  {
    "objectID": "posts/2022-01-03-data-types/index.html",
    "href": "posts/2022-01-03-data-types/index.html",
    "title": "data-types",
    "section": "",
    "text": "In the console, when you print a data frame it will give you the data type of each column displayed. This is helpful because it gives you a hint. I could not figure out a way to display a data frame with the the column types below but can display the output of the str function on the mtcars data set\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nYesterday I was working with the infer package and I was working through the examples and applying the examples to my own dataset. I could not figure out why the specify function would not work.\n\nI needed a factor variable and I had a factor variable but not the right type of factor variable.\n\nI had the data type <ord>and it was not working. The <ord> data type is an ordered factor.\nIt did not work to simply convert from an <ord> to a regular factor.\n\nquestions <- questions %>%\n  dplyr::mutate(last_year = as.factor(last_year))%>%\n\nIt stayed the same <ord>\nHowever this worked‚Äì I converted the variable to a character type and then convert it to a regular factor!\n\nquestions <- questions %>%\n  dplyr::mutate(last_year = as.character.factor(last_year))%>%\n  dplyr::mutate(last_year = as.factor(last_year)) %>%\n\nI will have update this post with some updated data."
  },
  {
    "objectID": "posts/2023-01-15-r--linux/index.html",
    "href": "posts/2023-01-15-r--linux/index.html",
    "title": "R & Linux",
    "section": "",
    "text": "Let me see if I recall this post when I need to re-install."
  },
  {
    "objectID": "posts/2022-01-01-docker-tutorial/index.html",
    "href": "posts/2022-01-01-docker-tutorial/index.html",
    "title": "Docker Tutorial",
    "section": "",
    "text": "I even had to change the Python version in the Docker file to make the Flask App work. When I received the original error‚ÄìI copied the error code and did a search. This post helped me realize the error was in fact in the code and not in the docker file."
  },
  {
    "objectID": "posts/2022-11-26-lazy--automation/index.html",
    "href": "posts/2022-11-26-lazy--automation/index.html",
    "title": "Lazy & Automation",
    "section": "",
    "text": "I have spent my weekend working with Google Apps Scripts to automate my work life. I wrote a script to draft an email with information from a google sheet. I think I will set up an automatic case note that logs to a Google Drive folder.\nThis post shows how to convert an html document to a blob object and save in a file format."
  },
  {
    "objectID": "posts/freelance/2020-06-19-freelance-posts.html",
    "href": "posts/freelance/2020-06-19-freelance-posts.html",
    "title": "freelance-posts",
    "section": "",
    "text": "Below is a simple analysis of my post from the Freelance Workshop. The workshop consisted of viewing a video by Seth Godin, and responding to question prompts and tagging . The key was writing your response and tagging other people to receive feedback and vice-versa.\nAt the end of the workshop, participants were able to download their posts into a csv file. I thought it may be good fun to analyze the data.\nI was surprised I only had 106 posts; it felt like much more and I am not sure had the capacity to write more.\n\n\n\n\nLoad Required packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(forcats)\nlibrary(viridis)\nlibrary(gt)\n\n\n\n\nBelow is a display of the first six rows and selected columns\n\npost %>%\n  dplyr::select(created_at, like_count, post) %>%\n  head() %>%\n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      created_at\n      like_count\n      post\n    \n  \n  \n    2020-03-30 20:56:58\n1\nFor kids: Adventures of Huckleberry Finn: Learn to see the world from many perspectives and learn empathy.\nFor adults: Lila An Inquiry Into Morals: Provides a framework for understanding life and change. \nFor people at work: The War of Art:  This book taught me how to work.\n    2020-03-31 01:21:54\n1\nThank you for the welcome @traviswilson . Good question, Finn and Lilla are about trying to understand people and society.\n    2020-03-31 20:37:27\n4\nHello! My name is Kevin Gilds residing in Lakeland, Florida--between Tampa and Orlando. I am here to learn how to take the next steps. I fell into my first freelancing gig and would love to do more.\n    2020-03-31 21:12:07\n2\nHi Razlan,\n\nI try to get the tough stuff done early while they are still asleep. We are fortunate that we have the flexibility to alternate care giving duties. The kids are also of the age where they are able to play together most of the time well enough. \n\nHang in there!\n    2020-03-31 23:53:03\n0\nThis is a great one; I let my former boss read and has not returned yet.... :upside_down_face:\n    2020-04-01 20:31:17\n3\nI am a huge fan of this podcast and blogging platform\n\nIt is from the creators of Basecamp.\n\nThe podcast:\nhttps://rework.fm/\n\nThe blogging/article platform\nhttps://m.signalvnoise.com/\n  \n  \n  \n\n\n\n\nI will use the tidytext package to break down the words in the post column and put it in a tidy format for analysis.\nLets take a peak\n\npost %>%\n  tidytext::unnest_tokens(word, post)%>%\n   dplyr::count(word) %>% # count number of words produces n\n   dplyr::arrange(desc(n)) %>%\n   dplyr::filter(n >=150) %>%\n    gt::gt() %>%\n      tab_header(\n    title = md(\"**Kevin's Top Words**\"),\n    subtitle = md(\"*The Freelance Workshop*\")\n  )\n\n\n\n\n\n  \n    \n      Kevin's Top Words\n    \n    \n      The Freelance Workshop\n    \n  \n  \n    \n      word\n      n\n    \n  \n  \n    to\n415\n    the\n344\n    i\n342\n    and\n237\n    a\n209\n    you\n202\n    is\n173\n    of\n161\n    this\n150\n  \n  \n  \n\n\n\n\nNot very interesting, but we can clean this up! The tidytext package has loaded stop words to help us remove common words, and I am going to add words specific to the workshop.\n\nComplete an anti-join to remove standard stop words and add your own custom words you wish to dismiss\n\nBelow are my custom words:\n\nword <- c(\"topic\",\"quote\", \"post\", \"1\", \"https\", \"hey\", \"2\", \"homeroom5\", \"5\", \"3\", \"6\", \"_kevin_\", \"_imposter_\", \"34974\", \"you‚Äôre\")\n\nworkshop_stop <- data.frame(word)\n\nBelow we have a script that adds some additional data processing to extract insights from the words. I added a case_when statement to deal with similiar words and the anti-join statement to remove non insightful words\n\nword_count <- post %>%\n  tidytext::unnest_tokens(word, post ) %>% #breakdown each word of the character vector\n  dplyr::mutate(word  = case_when( #make \n      word == \"client\" ~ \"clients\",\n      TRUE ~(word)\n    )) %>%  \n  dplyr::anti_join(stop_words) %>% #remove standard common words\n    dplyr::anti_join(workshop_stop) %>% #remove words specifc to the workshop that are not of valle\n  dplyr::count(word) %>% # count number of words produces n\n   dplyr::arrange(desc(n)) %>%\n   dplyr::filter(n >=16) %>%\n  gt::gt () %>%\n  tab_header(\n    title = md(\"**Kevin's Top Words**\"),\n    subtitle = md(\"*The Freelance Workshop*\")\n  )\n\nword_count\n\n\n\n\n\n  \n    \n      Kevin's Top Words\n    \n    \n      The Freelance Workshop\n    \n  \n  \n    \n      word\n      n\n    \n  \n  \n    data\n83\n    clients\n66\n    time\n29\n    evaluation\n26\n    people\n23\n    skills\n18\n    farid\n17\n    process\n16\n    story\n16\n  \n  \n  \n\n\n\n\nNow we are in position to graph the data. Below is a graph of my most frequent words with a mimimum of 16.\n\n\n\n\n\nCode for the graph is here:\nWhat were the most frequent post by category?\n\n\n\n\n\nCode for the graph is here:\nThis has been a basic introduction to text mining with R. It is possible to break down comments into their components. The most suprising word that stands out to me is time. It makes me want to re-read my post and understand the context better."
  },
  {
    "objectID": "posts/2022-01-09-nano/index.html",
    "href": "posts/2022-01-09-nano/index.html",
    "title": "Nano",
    "section": "",
    "text": "Let try this again"
  },
  {
    "objectID": "posts/2023-01-03-this-week-i-learned/index.html",
    "href": "posts/2023-01-03-this-week-i-learned/index.html",
    "title": "This week I learned",
    "section": "",
    "text": "How you have to flub up the first one to make the next one better but the point you have to have the first one to flub up. He provides a nice examples.\nI also think there is something about getting into flow state or the concept of shitty first drafts.\nWhen, I work on an analysis chart, table or blog post. I flub around at first and mess up the pancake."
  },
  {
    "objectID": "posts/2021-12-28-shiny-proxy-fun/index.html",
    "href": "posts/2021-12-28-shiny-proxy-fun/index.html",
    "title": "Shiny Proxy Fun",
    "section": "",
    "text": "This article got me thinking about other hosting possibilities such as html files and bookdown reports."
  },
  {
    "objectID": "posts/2023-04-05-interactive-documents/index.html",
    "href": "posts/2023-04-05-interactive-documents/index.html",
    "title": "Interactive Documents",
    "section": "",
    "text": "I confess I jump in first and then read the documentation. However, I am convinced the learning sinks in faster this way.\n\n\nI received a request for a dashboard. Um, not in the scope of work but I want to play with the interactive components of a Quarto documents.\nI jump in and I can‚Äôt not get the Server side to recognize the data. What I did not catch at first was the concept of Execution Contexts.\n\n#| context: setup\n#| include: false\n\n# load libraries\nlibrary(dplyr)\n\n# load data\ndataset <- import_data(\"data.csv\")\ndataset <- sample_n(dataset, 1000)\n\nI finally got my head around this was finally starting to display data. However, I was not thrilled on the output of the valueBoxes. Again, in the Quarto document it may not be the best context for them. Maybe use a general text based report.\nOne thing to consider will be to try and learn Observable JS. I am taking a class later this month."
  },
  {
    "objectID": "posts/bash/bash.html",
    "href": "posts/bash/bash.html",
    "title": "bash",
    "section": "",
    "text": "Using a bash script with Postgres database."
  },
  {
    "objectID": "posts/bash/bash.html#make-it-executable",
    "href": "posts/bash/bash.html#make-it-executable",
    "title": "bash",
    "section": "Make it executable",
    "text": "Make it executable\nchomd +x <filename>\nAt the top of the file add a shebang\n#!/bin/bash\n\nRead Data from CSV File\nThe script below reads the csv file and reads\ncat courses.csv | while IFS=\",\" read MAJOR COURSE\ndo\n  echo $MAJOR\ndone\n\n\nConnect to Database\nPSQL=\"psql -X --username=freecodecamp --dbname=students --no-align --tuples-only -c\"\n\n\nDatabase Variable\nMAJOR_ID=$($PSQL \"<query_here>\")\nMAJOR_ID=$($PSQL \"SELECT major_id FROM majors WHERE major='$MAJOR'\")\n\n\nCode Planning\nI liked this approach of using comments as a place holder for what to do next.\ndo\n  # get major_id\n\n  # if not found\n\n  # insert major\n\n  # get new major_id\n\n  # get course_id\n\n  # if not found\n\n  # insert course\n\n  # get new course_id\n\n  # insert into majors_courses\n\ndone"
  },
  {
    "objectID": "posts/2023-01-10-wilcoxsignedranktest/index.html",
    "href": "posts/2023-01-10-wilcoxsignedranktest/index.html",
    "title": "Wilcox-signed-rank-test",
    "section": "",
    "text": "A good question to consider is if I could make a graph like those in the post with more than 2 variables?"
  },
  {
    "objectID": "posts/2023-01-01-streatks/index.html",
    "href": "posts/2023-01-01-streatks/index.html",
    "title": "Streaks",
    "section": "",
    "text": "The year 2022 was a tough both professionally and personally. At times, they have blurred together. I feel like I am just drifting from issue to issue without any focus or strategy behind my actions. I feel a bit hopeless and lost.\nIn late June of 2022, I had an operating system crash and had to restore systems and pick back up. I lost momentum with a writing streak I had and it feels like I have stayed in survival mode since that time.\nI have tinkered with changing strategies to learn SQL and Postgress. There is a good course here.\nTo stay afloat at my day job; I have been learning Google App Script. It has been fun to pick up on some JavaScript methods. It also great that one can run some SQL in a Google Sheet. A great follow on YouTube to learn more about Google Sheets Query Function\n\n\nTo begin again is the key and play the game you wish to play. Here is an article from the Verge. Maybe personal blogging will catch on again; I just don‚Äôt think so.\n\n\n\nI think I need to write for myself and be hopeful someone may find it useful. The more I try to make it perfect the less likely it will ship.\n\n\n\nI made some changes and stole some theme settings; still can‚Äôt get a footer working but alas.\nHopefully you hear from me soon.\nThe end."
  },
  {
    "objectID": "posts/2021-06-16-side-benefits-of-learning-r/index.html",
    "href": "posts/2021-06-16-side-benefits-of-learning-r/index.html",
    "title": "Side Benefits of Learning R",
    "section": "",
    "text": "In my experience learning R has opened up many other content areas for me. Granted there are many paths to learning new things but I am not sure how I get to my list below if I stayed with a learning path emphasizing spreadsheets.\n\nLinux\nGit\nWeb-development\nCSSS\nHTML"
  },
  {
    "objectID": "posts/2021-08-31-lessons/index.html",
    "href": "posts/2021-08-31-lessons/index.html",
    "title": "Lessons",
    "section": "",
    "text": "Every August after a big grant report is complete, I update R and my packages and that can be a pain but usually not too bad. However, last year there was the crashed laptop but that was an Ubuntu version update.\nThis year I learned the importance of the renv üì¶. I had an important Shiny Application fail because of the changes in the bs4Dash. (Yes, I recall reading the warnings but I forgot).\nHonestly, I was intimidated to try the renv package because of trying packratüì¶ in the past Thus far, it does not seem to hard to work with."
  },
  {
    "objectID": "posts/2021-12-19-til/index.html",
    "href": "posts/2021-12-19-til/index.html",
    "title": "TIL",
    "section": "",
    "text": "I am not sure why I could not recall how to filter multiple values under on variable but here we are! The slice function is also becoming one of my favorite tools.\npre_numeric <- pre_summary %>%\n  dplyr::filter(skim_type == \"numeric\") %>%\n  dplyr::slice_tail(n = 6) %>%\n  dplyr::filter(skim_variable == \"var2_avg\" |\n                  skim_variable == \"var1_avg\" |\n                  skim_variable == \"var3_avg\") \n                  \nIn this function, I have to make sure that NA values are converted to zero before I can determine success or failure. The df has to be called first and then start the new dplyr statement.\nget_var_score <- function(df){\n  \n    df <- dplyr::mutate(df, var_adjusted = 4 - sc1) \n    df <- dplyr::mutate(df, var_avg = sc0 / var_adjusted) \n    df$var_avg <- tidyr::replace_na(df$var_avg, 0)\n    df \n    dplyr::mutate(df, var_success = if_else(var_avg >= 4.45, TRUE, FALSE))\n    \n    \n}\nHere I needed to actually filter navalues in this column to see troubled records.\nissue_13 <- survey %>%\n  dplyr::filter(is.na(initials_1))"
  },
  {
    "objectID": "posts/2022-01-07-commit-to-the-static/index.html",
    "href": "posts/2022-01-07-commit-to-the-static/index.html",
    "title": "commit to the static",
    "section": "",
    "text": "You need commit the static folder to get plots to render in blogdown.post"
  },
  {
    "objectID": "posts/2022-01-08-git-hooks/index.html",
    "href": "posts/2022-01-08-git-hooks/index.html",
    "title": "Git Hooks",
    "section": "",
    "text": "I came here for an article on a .gitlab-ci.yml example and came away with an article on githooks. I think the githook is what I need!"
  },
  {
    "objectID": "posts/remedy/addins.html",
    "href": "posts/remedy/addins.html",
    "title": "addins",
    "section": "",
    "text": "I downloaded the remedy package today."
  },
  {
    "objectID": "posts/2022-01-04-a-readme-tale/index.html",
    "href": "posts/2022-01-04-a-readme-tale/index.html",
    "title": "A README Tale",
    "section": "",
    "text": "today i share some #rstats adventures that challenged me üßómost of it was probably self-inflicted from not reading documentation thoroughly üò¨rmarkdown::render(‚ÄúREADME.Rmd‚Äù, ‚Äúgithub_document‚Äù) in GitHub Actions really got me tho ü•¥https://t.co/FcqAS43i2o\n\n‚Äî Shannon Pileggi (@PipingHotData) December 13, 2021\n\n\nI loved the tweet because I wanted to follow up on the automated testing components. I am hoping to still get there, but thus far I have learned a lot on the web scraping elements of my website and see some other possibilities. It will need a weekend post to illuminate more‚Ä¶‚Ä¶‚Ä¶‚Ä¶\nHowever, check out the README for this site."
  },
  {
    "objectID": "posts/2021-12-30-docker/index.html",
    "href": "posts/2021-12-30-docker/index.html",
    "title": "Docker",
    "section": "",
    "text": "Docker Deep Dive was recommended to me yesterday. I start using Docker and then I stop and lose the momentum. Kind of like programming, I suspect you have to do Docker in order to learn it. I just need to do it and hopefully this book can help me sustain."
  },
  {
    "objectID": "posts/2021-09-24-communication-toolbox-with-shiny/index.html",
    "href": "posts/2021-09-24-communication-toolbox-with-shiny/index.html",
    "title": "Communication Toolbox with Shiny",
    "section": "",
    "text": "Validation\nI depend on the validation function in shiny to communicate to my users when there is no data entry for a given grant or within a given time frame.\noutput$grant_level_tbl <- DT::renderDT({\n      tbl <- source_level()\n      if(source_level() %>% nrow() < 1) {\n        validate(\"Sorry, no data available\") ##the order of this matters\n      }\n      tbl <- outcome_tbl(tbl) %>%\n        janitor::adorn_ns() %>%\n      DT::datatable(tbl, rownames = FALSE,\n                    extensions = \"Buttons\",\n                    options = list(\n                      dom = 'tB',\n                      buttons = c('copy', 'print', 'csv')\n                    ))\n    })\nThe source_level() is reactive data and given input there may be hundreds of data points or none. My validation line states if the reactive value has less than 1 row the following message is displayed\n\nSorry, no data available.\n\nThis communicates to the program manager that surveys have not been entered yet. Most likely this is expected, but the message communicates that there is no data here to display.\nThe alternative to a validation statement is a blank graph canvas and a ugly error message with table the output.\n\n\nConclusion\nI was thinking of the validation function in terms of a practical tool but I see the importance of it as a communication tool as well. What I enjoy most about participating in the book club, is picking up detail and the opportunity for reflection."
  },
  {
    "objectID": "posts/2022-11-12-reprex/index.html",
    "href": "posts/2022-11-12-reprex/index.html",
    "title": "Reprex",
    "section": "",
    "text": "II Other Factor Stuff\nA good video on generating crosstab reports in R.\nhttps://www.infoworld.com/video/104862/how-to-create-crosstab-reports-in-r\nA couple of interesting packages to check out\nThese looks like a promising data vis package vtree and the CGPFunctions"
  },
  {
    "objectID": "posts/2021-09-14-learning/index.html",
    "href": "posts/2021-09-14-learning/index.html",
    "title": "Shaking up Mental Models",
    "section": "",
    "text": "Once you see it; you can‚Äôt unsee it.ü§î\n\nOne benefit of learning a new framework is that it gives you the opportunity to compare and contrast with an established mental model.\nI have been taking a Pandas Course from Kaggle and the lesson in Pandas involved establishing new variables in order to extract information from the data frame.\n\nindices = [0, 1, 10, 100]\nvar = ['country', 'province', 'region_1', 'region_2']\ndf = reviews.loc[indices, var]\n\nThis seemed like a pain in order to select certain rows and columns, but it did open my perspective to a challenge I was having.\nI have been working on reading data from a Qualtrics survey and there are nearly 147 columns and only about 117 are needed. (Long story on templated survey tools). To parse the data frame down, I had been using indexes for selections. Using an index is okay but frustrating as you are testing because the index selection breaks when there is a change to the survey. It was also a pain to write out all those terrible column names. The python script above made me think to create a vector to reference in a select statement.\n\nIs this possible‚Äìyes it is, and now I seem to see it everywhere.\n\nBelow is a minimal example with the ‚Äòmtcars‚Äô data set.\n\nremove <- mtcars %>%\n  dplyr::select(drat, wt, qsec)\n\nremove <- names(remove) #create a vector with the names of the columns you eventually want to exclude\n\n\nnew_mtcars <- mtcars %>%\n  dplyr::select(-all_of(remove))  #within the select statement us the helper 'all_of' with the - operator to deselect the vector of interest. \n\n\nnew_mtcars\n\n                     mpg cyl  disp  hp vs am gear carb\nMazda RX4           21.0   6 160.0 110  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110  0  1    4    4\nDatsun 710          22.8   4 108.0  93  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175  0  0    3    2\nValiant             18.1   6 225.0 105  1  0    3    1\nDuster 360          14.3   8 360.0 245  0  0    3    4\nMerc 240D           24.4   4 146.7  62  1  0    4    2\nMerc 230            22.8   4 140.8  95  1  0    4    2\nMerc 280            19.2   6 167.6 123  1  0    4    4\nMerc 280C           17.8   6 167.6 123  1  0    4    4\nMerc 450SE          16.4   8 275.8 180  0  0    3    3\nMerc 450SL          17.3   8 275.8 180  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205  0  0    3    4\nLincoln Continental 10.4   8 460.0 215  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230  0  0    3    4\nFiat 128            32.4   4  78.7  66  1  1    4    1\nHonda Civic         30.4   4  75.7  52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65  1  1    4    1\nToyota Corona       21.5   4 120.1  97  1  0    3    1\nDodge Challenger    15.5   8 318.0 150  0  0    3    2\nAMC Javelin         15.2   8 304.0 150  0  0    3    2\nCamaro Z28          13.3   8 350.0 245  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175  0  0    3    2\nFiat X1-9           27.3   4  79.0  66  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91  0  1    5    2\nLotus Europa        30.4   4  95.1 113  1  1    5    2\nFord Pantera L      15.8   8 351.0 264  0  1    5    4\nFerrari Dino        19.7   6 145.0 175  0  1    5    6\nMaserati Bora       15.0   8 301.0 335  0  1    5    8\nVolvo 142E          21.4   4 121.0 109  1  1    4    2\n\n\n\nConclusion\nLearning python helped me shake up my mental model and apply it to my R workflow."
  },
  {
    "objectID": "posts/2022-12-16-data-types/index.html",
    "href": "posts/2022-12-16-data-types/index.html",
    "title": "Data Types",
    "section": "",
    "text": "I use the case_when function to clean up data entry issues and I was stumped for a bit when my common structure did not work.\nI am prone to messing up a case_when statements but I was getting frustrated after a few failures and non helpful error messages. I isolated the problem down and realized the pattern is that the case_when statement was not working on ordinal level variables. I changed the data type of these variables to character and was back in business.\nCheck your data types when a function or script is not working as expected."
  },
  {
    "objectID": "posts/foreign-keys/sql-foreign-keys.html",
    "href": "posts/foreign-keys/sql-foreign-keys.html",
    "title": "foreign keys",
    "section": "",
    "text": "It drives me batty not to use commas and appreciate the parantheses.\nALTER TABLE <table_name> ADD FOREIGN KEY(<column_name>)\nREFERENCES <referenced_table_name>(<referenced_column_name>);"
  },
  {
    "objectID": "posts/2022-11-15-likert-scales/index.html",
    "href": "posts/2022-11-15-likert-scales/index.html",
    "title": "Likert Scales",
    "section": "",
    "text": "The book reminded me of the likert package and to utilize the Mann-Whitney test. What I found interesting in his approach to visualization was that the numbers are visualized instead of the category labels. I have always plotted the category labels and making the graph too busy. Thinking out loud‚Äìit may be useful for me to plot by numbers but not my intended audience.\nOther resources I gave gather up over the past few days include the following:\nVideos: Crosstab video\nPackages:\nvtree packages\nCGPfunctions"
  },
  {
    "objectID": "posts/2022-01-02-flextable/index.html",
    "href": "posts/2022-01-02-flextable/index.html",
    "title": "flextable",
    "section": "",
    "text": "I am preparing to complete a report using officedown due at the end of the month. I was checking out the package out today and was happy to see I could at least open the output file in a LibreOffice Writer. I was worried about tables in the document but it looks like flextable seems to produce great looking tables and seems pretty straightforward to implement."
  },
  {
    "objectID": "posts/2022-11-10-make-terrible-gadgets/index.html",
    "href": "posts/2022-11-10-make-terrible-gadgets/index.html",
    "title": "Make terrible gadgets",
    "section": "",
    "text": "To make a good gadgets, you have to make some terrible ones. Actually the one I just made seems to have worked very well. Other than I need to load my own package, Shiny and MiniUI--but I think that is fixable."
  },
  {
    "objectID": "posts/2021-12-23-writing-in-community/index.html",
    "href": "posts/2021-12-23-writing-in-community/index.html",
    "title": "Writing in Community",
    "section": "",
    "text": "Books topics\n\nMetallica and the practice\nAutobiography\nNonprofit Evaluation."
  },
  {
    "objectID": "posts/r-packages/r-packages.html",
    "href": "posts/r-packages/r-packages.html",
    "title": "R-packages",
    "section": "",
    "text": "The QualtRics pakcage is a key r package I utilize. The package provides an API access from Qualtrics to a R environment. If you manage separate Qualtrics accounts, set your credentials to project environmental settings. See below\n\n usethis::edit_r_environ(\"project\")"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html",
    "href": "posts/maps/2018-05-09-maps.html",
    "title": "Maps",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.3.6     ‚úî purrr   0.3.4\n‚úî tibble  3.1.8     ‚úî dplyr   1.0.9\n‚úî tidyr   1.2.0     ‚úî stringr 1.4.0\n‚úî readr   2.1.2     ‚úî forcats 0.5.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(httr)\nlibrary(ggmap)\n\nGoogle's Terms of Service: https://cloud.google.com/maps-platform/terms/.\nPlease cite ggmap if you use it! See citation(\"ggmap\") for details.\n\nlibrary(here)\n\nhere() starts at /home/kgilds/kevinsblogIII"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#get-the-data",
    "href": "posts/maps/2018-05-09-maps.html#get-the-data",
    "title": "Maps",
    "section": "Get the Data",
    "text": "Get the Data\n\nurl <- 'https://www.fldoe.org/core/fileparse.php/7584/urlt/1516ABS21DAYSTDistSchl.xlsx' #location of data on the internet\n\n\n#httr::GET(url, write_disk(absences <- tempfile(fileext = \".xls\"))) #obtain the spreadsheet file from the internet"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#read-the-data",
    "href": "posts/maps/2018-05-09-maps.html#read-the-data",
    "title": "Maps",
    "section": "Read the data",
    "text": "Read the data\n\nabsences_2 <- readxl::read_excel(\nhere(\"posts\", \"maps\", \"absences.xlsx\"),\n     skip =2) #read the spreadsheet file and skip the first two rows\n\n\n\n\n\nabsences_2 <- janitor::clean_names(absences_2) #Change column names with the Janitor.\n\n\nabsences_2 <- dplyr::rename(absences_2, \"absent_21_plus\" = \"absent_21_days_or_over\") #Change name again to shorten\n\nabsences_2 <- dplyr::select(absences_2, 2, 4:5) #select columsn of interest"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#clean-and-convert-the-data",
    "href": "posts/maps/2018-05-09-maps.html#clean-and-convert-the-data",
    "title": "Maps",
    "section": "Clean and Convert the data",
    "text": "Clean and Convert the data\n\n#absences_2$enrollments <- as.numeric(absences_2$enrollments) #Change data to numeric format\nabsences_2$absent_21_plus <- as.numeric(absences_2$absent_21_plus) #change data to numeric format"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#make-my-own-percents",
    "href": "posts/maps/2018-05-09-maps.html#make-my-own-percents",
    "title": "Maps",
    "section": "Make my own percents",
    "text": "Make my own percents\n\n#absences_2 <- dplyr::mutate(absences_2, percent = absent_21_plus / enrollments) \n\n\n#calculate percent \nabsences_2 <- dplyr::mutate(absences_2, percent = percent_absent_21_or_more_days *100) #Convert from decimal"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#summary-stats-for-plotting",
    "href": "posts/maps/2018-05-09-maps.html#summary-stats-for-plotting",
    "title": "Maps",
    "section": "Summary Stats for plotting",
    "text": "Summary Stats for plotting\n\nabsent_21 <- absences_2 %>%\n  group_by(district_name) %>%\n  summarise(mean = mean(percent, na.rm = TRUE))\n\n\n\nabsent_21$mean <- round(absent_21$mean, 2)\n\nabsent_21\n\n# A tibble: 75 √ó 2\n   district_name  mean\n   <chr>         <dbl>\n 1 ALACHUA       13.9 \n 2 BAKER         21.0 \n 3 BAY           12.7 \n 4 BRADFORD      18.6 \n 5 BREVARD        6.62\n 6 BROWARD        8.71\n 7 CALHOUN       20.9 \n 8 CHARLOTTE      7.58\n 9 CITRUS        11.0 \n10 CLAY           9.78\n# ‚Ä¶ with 65 more rows"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#maps",
    "href": "posts/maps/2018-05-09-maps.html#maps",
    "title": "Maps",
    "section": "Maps",
    "text": "Maps\n\nGet Florida Map\n\nstates <- map_data(\"state\")\nfl_df <- subset(states, region == \"florida\")\n\nThis chunks sets up how to overlay county boundaries over the Florida map.\n\ncounties <- map_data(\"county\")\nfl_county <- subset(counties, region == \"florida\")\n\nThis will us a map of Florida\n\nfl_base <- ggplot(data = fl_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.3) + \n  geom_polygon(color = \"black\", fill = \"gray\")\nfl_base + theme_nothing()\n\n\n\n\n\nfl_base + theme_nothing() + \n  geom_polygon(data = fl_county, fill = NA, color = \"white\") +\n  geom_polygon(color = \"black\", fill = NA)  # get the state border back on top"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#back-to-the-educational-set",
    "href": "posts/maps/2018-05-09-maps.html#back-to-the-educational-set",
    "title": "Maps",
    "section": "Back to the educational set",
    "text": "Back to the educational set\nSchool districts in Florida pretty much align with Florida Counties. However, there is nuance between the map data and the school district data.\nThis chunk renames the district_name to match the name in the map data and matches the case.\n\nabsent_21_c <- absent_21 %>%\n  rename(\"subregion\" = \"district_name\") #match name with map data in preperation of joining. \n\nabsent_21_c$subregion <- tolower(absent_21_c$subregion) #make lowercase\n\nabsent_21_c\n\n# A tibble: 75 √ó 2\n   subregion  mean\n   <chr>     <dbl>\n 1 alachua   13.9 \n 2 baker     21.0 \n 3 bay       12.7 \n 4 bradford  18.6 \n 5 brevard    6.62\n 6 broward    8.71\n 7 calhoun   20.9 \n 8 charlotte  7.58\n 9 citrus    11.0 \n10 clay       9.78\n# ‚Ä¶ with 65 more rows"
  },
  {
    "objectID": "posts/maps/2018-05-09-maps.html#use-the-anti-join-to-determine-issues-with-merging",
    "href": "posts/maps/2018-05-09-maps.html#use-the-anti-join-to-determine-issues-with-merging",
    "title": "Maps",
    "section": "Use the anti-join to determine issues with merging",
    "text": "Use the anti-join to determine issues with merging\n\nnon_match <- anti_join(fl_county, absent_21_c, by = \"subregion\")\n\nnon_match\n\n         long      lat group order  region  subregion\n1   -80.89018 25.79456   302 12629 florida miami-dade\n2   -80.89018 25.97218   302 12630 florida miami-dade\n3   -80.69538 25.96645   302 12631 florida miami-dade\n4   -80.68965 25.94926   302 12632 florida miami-dade\n5   -80.30003 25.94926   302 12633 florida miami-dade\n6   -80.30003 25.96072   302 12634 florida miami-dade\n7   -80.13387 25.96072   302 12635 florida miami-dade\n8   -80.13961 25.90342   302 12636 florida miami-dade\n9   -80.12814 25.85186   302 12637 florida miami-dade\n10  -80.12814 25.81748   302 12638 florida miami-dade\n11  -80.12814 25.79456   302 12639 florida miami-dade\n12  -80.13387 25.78310   302 12640 florida miami-dade\n13  -80.14534 25.77737   302 12641 florida miami-dade\n14  -80.16252 25.78310   302 12642 florida miami-dade\n15  -80.16825 25.80029   302 12643 florida miami-dade\n16  -80.16825 25.82321   302 12644 florida miami-dade\n17  -80.16252 25.85759   302 12645 florida miami-dade\n18  -80.17398 25.85759   302 12646 florida miami-dade\n19  -80.18545 25.85186   302 12647 florida miami-dade\n20  -80.19691 25.83467   302 12648 florida miami-dade\n21  -80.20263 25.78883   302 12649 florida miami-dade\n22  -80.20836 25.75445   302 12650 florida miami-dade\n23  -80.21982 25.73726   302 12651 florida miami-dade\n24  -80.25420 25.72008   302 12652 florida miami-dade\n25  -80.26566 25.69716   302 12653 florida miami-dade\n26  -80.27712 25.63986   302 12654 florida miami-dade\n27  -80.30003 25.62840   302 12655 florida miami-dade\n28  -80.31149 25.61694   302 12656 florida miami-dade\n29  -80.32296 25.59975   302 12657 florida miami-dade\n30  -80.32296 25.56538   302 12658 florida miami-dade\n31  -80.32296 25.54819   302 12659 florida miami-dade\n32  -80.33441 25.52527   302 12660 florida miami-dade\n33  -80.35160 25.49662   302 12661 florida miami-dade\n34  -80.35733 25.46225   302 12662 florida miami-dade\n35  -80.34587 25.42214   302 12663 florida miami-dade\n36  -80.32868 25.39349   302 12664 florida miami-dade\n37  -80.32868 25.37057   302 12665 florida miami-dade\n38  -80.33441 25.35338   302 12666 florida miami-dade\n39  -80.38598 25.33046   302 12667 florida miami-dade\n40  -80.39171 25.30182   302 12668 florida miami-dade\n41  -80.40317 25.29036   302 12669 florida miami-dade\n42  -80.42609 25.27317   302 12670 florida miami-dade\n43  -80.43755 25.23879   302 12671 florida miami-dade\n44  -80.44328 25.22160   302 12672 florida miami-dade\n45  -80.47192 25.23306   302 12673 florida miami-dade\n46  -80.48911 25.22733   302 12674 florida miami-dade\n47  -80.50057 25.20441   302 12675 florida miami-dade\n48  -80.51776 25.20441   302 12676 florida miami-dade\n49  -80.55787 25.23306   302 12677 florida miami-dade\n50  -80.58651 25.23879   302 12678 florida miami-dade\n51  -80.59798 25.23306   302 12679 florida miami-dade\n52  -80.59225 25.20441   302 12680 florida miami-dade\n53  -80.60944 25.19296   302 12681 florida miami-dade\n54  -80.63235 25.19296   302 12682 florida miami-dade\n55  -80.67818 25.18150   302 12683 florida miami-dade\n56  -80.71256 25.15858   302 12684 florida miami-dade\n57  -80.79278 25.15858   302 12685 florida miami-dade\n58  -80.82716 25.17576   302 12686 florida miami-dade\n59  -80.85007 25.18150   302 12687 florida miami-dade\n60  -80.89591 25.18150   302 12688 florida miami-dade\n61  -80.89018 25.79456   302 12689 florida miami-dade\n62  -81.56627 27.03215   303 12691 florida    de soto\n63  -82.05329 27.03215   303 12692 florida    de soto\n64  -82.05329 27.10090   303 12693 florida    de soto\n65  -82.21371 27.10090   303 12694 florida    de soto\n66  -82.21371 27.19831   303 12695 florida    de soto\n67  -82.15642 27.20403   303 12696 florida    de soto\n68  -82.15642 27.24414   303 12697 florida    de soto\n69  -82.05901 27.24987   303 12698 florida    de soto\n70  -82.05901 27.33582   303 12699 florida    de soto\n71  -81.56054 27.33582   303 12700 florida    de soto\n72  -81.56627 27.03215   303 12701 florida    de soto\n73  -81.57200 29.82245   345 14185 florida   st johns\n74  -81.59492 29.90267   345 14186 florida   st johns\n75  -81.58920 29.93704   345 14187 florida   st johns\n76  -81.60065 29.96569   345 14188 florida   st johns\n77  -81.66940 30.02299   345 14189 florida   st johns\n78  -81.66940 30.04591   345 14190 florida   st johns\n79  -81.66367 30.07456   345 14191 florida   st johns\n80  -81.66367 30.09174   345 14192 florida   st johns\n81  -81.66367 30.11466   345 14193 florida   st johns\n82  -81.63503 30.11466   345 14194 florida   st johns\n83  -81.62357 30.11466   345 14195 florida   st johns\n84  -81.59492 30.11466   345 14196 florida   st johns\n85  -81.55482 30.09748   345 14197 florida   st johns\n86  -81.53762 30.09174   345 14198 florida   st johns\n87  -81.53189 30.08601   345 14199 florida   st johns\n88  -81.43449 30.08601   345 14200 florida   st johns\n89  -81.42876 30.24644   345 14201 florida   st johns\n90  -81.36574 30.24644   345 14202 florida   st johns\n91  -81.35427 30.18915   345 14203 florida   st johns\n92  -81.33136 30.09748   345 14204 florida   st johns\n93  -81.29125 29.96569   345 14205 florida   st johns\n94  -81.27979 29.91986   345 14206 florida   st johns\n95  -81.29125 29.87975   345 14207 florida   st johns\n96  -81.26833 29.86256   345 14208 florida   st johns\n97  -81.25114 29.83391   345 14209 florida   st johns\n98  -81.25114 29.78808   345 14210 florida   st johns\n99  -81.24541 29.72505   345 14211 florida   st johns\n100 -81.23396 29.70213   345 14212 florida   st johns\n101 -81.21104 29.69067   345 14213 florida   st johns\n102 -81.19958 29.67348   345 14214 florida   st johns\n103 -81.19958 29.64484   345 14215 florida   st johns\n104 -81.25114 29.65056   345 14216 florida   st johns\n105 -81.29125 29.64484   345 14217 florida   st johns\n106 -81.30844 29.65056   345 14218 florida   st johns\n107 -81.31990 29.64484   345 14219 florida   st johns\n108 -81.31417 29.62765   345 14220 florida   st johns\n109 -81.52616 29.62192   345 14221 florida   st johns\n110 -81.52616 29.75370   345 14222 florida   st johns\n111 -81.53189 29.76516   345 14223 florida   st johns\n112 -81.57200 29.82245   345 14224 florida   st johns\n113 -80.33441 27.54781   346 14226 florida   st lucie\n114 -80.32868 27.50770   346 14227 florida   st lucie\n115 -80.32868 27.46760   346 14228 florida   st lucie\n116 -80.31149 27.43322   346 14229 florida   st lucie\n117 -80.27139 27.35301   346 14230 florida   st lucie\n118 -80.24274 27.29571   346 14231 florida   st lucie\n119 -80.21409 27.25560   346 14232 florida   st lucie\n120 -80.28857 27.25560   346 14233 florida   st lucie\n121 -80.28857 27.20403   346 14234 florida   st lucie\n122 -80.67245 27.20403   346 14235 florida   st lucie\n123 -80.67245 27.55354   346 14236 florida   st lucie\n124 -80.33441 27.54781   346 14237 florida   st lucie\n\n\nNot all the names match; so a touchup of subregion names is called for.\n\nabsent_21_c$subregion <- gsub(\"dade\", \"miami-dade\", fixed = TRUE, absent_21_c$subregion)\n\nabsent_21_c$subregion <- gsub(\"desoto\", \"de soto\", fixed = TRUE, absent_21_c$subregion)\n\nabsent_21_c$subregion <- gsub(\"st. johns\", \"st johns\", fixed = TRUE, absent_21_c$subregion)\n\nabsent_21_c$subregion <- gsub(\"st. lucie\", \"st lucie\", fixed = TRUE, absent_21_c$subregion)\n\n\nJoin Forces\nPut the\n\nmap_d <- inner_join(fl_county, absent_21_c, by = \"subregion\") #join map data and educational data"
  },
  {
    "objectID": "posts/2021-12-27-automated-testing-links/index.html",
    "href": "posts/2021-12-27-automated-testing-links/index.html",
    "title": "Automated Testing Links",
    "section": "",
    "text": "I have been looking for this article; the article covers automated testing and deploying a Shiny Application. I suspect one could write the manifest to send to shinyapps.io.\nI liked the tips and tricks of this post"
  },
  {
    "objectID": "posts/2021-12-18-abstraction/index.html",
    "href": "posts/2021-12-18-abstraction/index.html",
    "title": "Abstraction",
    "section": "",
    "text": "Function based approach\n\nI was once had an analysis pipeline that I compared to a worn down car. You turn it on in the morning and not confident it was really going to start or if it did start was it going to leave you on the side of the road.\n\nMy analysis pipelines used to be long complicated R scripts but I have transitioned to using functions within script files and or Rmd files. What got me on this road was the discussion around the Drake/Targets Package.\nThe first time someone paid me to develop a Shiny Application the application was a beast with thousands of lines of code and awful to maintain.\nI tried using modules but this did not work out.\nThankfully, I was soon introduced to the Golem Package and started modularzing my applications."
  },
  {
    "objectID": "posts/2023-01-02-today-i-learned/index.html",
    "href": "posts/2023-01-02-today-i-learned/index.html",
    "title": "Today I learned",
    "section": "",
    "text": "I was making more difficult. I need to extract the month from dates submitted.\ntest_date <- session_summary %>% \n  dplyr::mutate(date = as.Date(date)) %>% \n  dplyr::mutate(date = lubridate::ymd(date)) %>% \n  dplyr::mutate(month = lubridate:: month(date, label = TRUE))\nThe worst is when the code runs and appears correct but is not.\nTurns out I was making the process too difficult here is the code chunk that worked.\nplay_session <- session_summary %>% \n  dplyr::mutate(new_date = lubridate::mdy(date)) %>%\n  dplyr::mutate(month = lubridate:: month(new_date, label = TRUE))\nMaybe not optimized but it worked.\nUntil next time, cheers!"
  },
  {
    "objectID": "posts/2022-01-05-column-names/index.html",
    "href": "posts/2022-01-05-column-names/index.html",
    "title": "Column Names",
    "section": "",
    "text": "I saw this thread today\n\n\nHave you seen @EmilyRiederer‚Äôs work on column names? https://t.co/FN05q3SzIg\n\n‚Äî Jenna Jordan (@JennaJrdn) January 5, 2022\n\n\nI recall reading this post in the past and I am sure I did not absorb the significance of it the first time. This is design applied to data management! ü§ò"
  },
  {
    "objectID": "posts/2021-07-23-where-are-my-keys/index.html",
    "href": "posts/2021-07-23-where-are-my-keys/index.html",
    "title": "Where are my keys",
    "section": "",
    "text": "Happy Git with R.\nGreat Link\n\nThe problem:\nGithub was asking me a password when I have keys set up.\nStep 1:\nIn the terminal\ngit remote -v \nOkay the terminal is telling me that the remote is set to https:\nhttps://github.com/USERNAME/REPOSITORY.git\n\nThe fix\ngit remote set-url origin git@github.com:USERNAME/REPOSITORY.git\n\n\nFinish\nMy issue is that I was forgetting to include the origin."
  },
  {
    "objectID": "posts/2019-03-27-Pets/index.html",
    "href": "posts/2019-03-27-Pets/index.html",
    "title": "Pets",
    "section": "",
    "text": "Conclusion\nI played with the five_thirtyeight theme and it worked well. I think it would work better with some colors in the bar plot but I am not sure how I would want to color this graph."
  },
  {
    "objectID": "posts/move/move.html",
    "href": "posts/move/move.html",
    "title": "move",
    "section": "",
    "text": "I moved to Quarto and not sure how I feel about it. I know how Blogdown and Hugo can be burdensome. I just kept waiting for things to break on me. I miss my add new post Addins but I guess I should just make it myself. I will slowly port over worthwhile post to this new version of the site."
  },
  {
    "objectID": "posts/2021-11-15-testing-shiny-applications/index.html",
    "href": "posts/2021-11-15-testing-shiny-applications/index.html",
    "title": "Testing Shiny Applications",
    "section": "",
    "text": "However, I could not get a good grasp of unit tests for a shiny application. I was reading the Mastering Shiny Book and still could not grasp it. I did find this article that got me going. The exercise of running the script below and playing with the application and tests helped me understand the testing process for Shiny Applications.\nshinyAppTemplate(\"myapp\")\nI was able to implement a couple of simple automated tests in my own shiny applications. The only roadblock I ran into is that I had not exported my modules and the testing engine could not find the server I was testing.\nI start with simple tests and work towards more complexity. My favorite basic tests is to count the number of columns. Here is the form:\ntestServer(mod_home_server, {\n\nexpect_equal(ncol(sites()), 4)\n\n})\n\ntestServer(mod_home_server, {\n\nexpect_equal(colnames(program()), c(\"time\", \"site\", \"grant\", \"id_code\"))\n\n})"
  },
  {
    "objectID": "posts/2021-10-22-data-management/index.html",
    "href": "posts/2021-10-22-data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "What I like about both articles is that they are both:\n\nPractical\nApproachable for non-technical users. I feel I could share these articles and managers of social service providers could understand them.\n\nI started writing a data cleaning plan, and I am pleased I started the process. Writing out the plan provided a sense of relief to have it out of my head and into documentation. It also struck me that much of my data cleaning should be in my Readme file."
  },
  {
    "objectID": "posts/2021-12-21-maps/index.html",
    "href": "posts/2021-12-21-maps/index.html",
    "title": "Maps!",
    "section": "",
    "text": "Here is a tutorial on using the mapview package. I hope I get to use it soon."
  },
  {
    "objectID": "posts/2022-11-16-codebooks-and-more/index.html",
    "href": "posts/2022-11-16-codebooks-and-more/index.html",
    "title": "Codebooks and More",
    "section": "",
    "text": "As I was starting to work on a data dictionary for a project, I discovered the get_information_report function from the pointblank` package this morning. I love the output."
  },
  {
    "objectID": "posts/2021-12-11-dashboards/index.html",
    "href": "posts/2021-12-11-dashboards/index.html",
    "title": "Dashboards",
    "section": "",
    "text": "About this time, I ran into this article about dashboards. It is a thoughtful piece on the context of dashboard troubles and when they can be useful.\nIt is wonderful when you can build a dashboard that is useful for someone even for a specific purpose. The article also serves as a reminder to keep things simple and not to get hung up on all encompassing tool."
  },
  {
    "objectID": "posts/2021-12-08-2021-12-eval-twitter-threads/index.html",
    "href": "posts/2021-12-08-2021-12-eval-twitter-threads/index.html",
    "title": "2021-12-Eval-Twitter-Threads",
    "section": "",
    "text": "2021-12-07\n\n\nDon‚Äôt begin any evaluation work without an official / formal contract from your client. (Repeat after me.)\n\n‚Äî Kavita Mittapalli, Ph.D.¬†(@KavitaMNA) December 7, 2021\n\n\n\n\nLet‚Äôs talk about contracts or the lack thereof. Here‚Äôs a short piece I just wrote/posted on my website and LinkedIn: https://t.co/70Z6MekAqD#contracts #evaluation #grants #smallbusiness #consultants\n\n‚Äî Kavita Mittapalli, Ph.D.¬†(@KavitaMNA) December 8, 2021"
  },
  {
    "objectID": "posts/sql_cert/sql_cert.html",
    "href": "posts/sql_cert/sql_cert.html",
    "title": "sql cert",
    "section": "",
    "text": "What I have liked thus far is that the courses emphasize using the command line and using Bash. The courses have been well designed. The skills build on each other and there is a lot of repetition to reinforce concepts. The hints in the course also provide an opportunity for the student to work out the problem before the solution is provided."
  },
  {
    "objectID": "posts/2021-12-17-to-make-upper-case/index.html",
    "href": "posts/2021-12-17-to-make-upper-case/index.html",
    "title": "To make upper case",
    "section": "",
    "text": "I needed to make the values in a column consistent and was struggling to add in the base R function of toupper in a dplyr pipeline. I found the str_to_upper(string, locale = \"en\") function in the stringr package and made it work."
  },
  {
    "objectID": "posts/2021-12-14-fun-with-readme/index.html",
    "href": "posts/2021-12-14-fun-with-readme/index.html",
    "title": "Fun with Readme & Github Actions",
    "section": "",
    "text": "I have been focused on github actions and gitlab pipelines for about two weeks, and I stumbled upon this cool post. Not only is there good content on Github Actions, but content and script for the README is awesome. In addition, numerous posts on using automated workflows and other awesome content."
  },
  {
    "objectID": "posts/2021-10-30-file-paths/index.html",
    "href": "posts/2021-10-30-file-paths/index.html",
    "title": "File-Paths",
    "section": "",
    "text": "I had forgotten about the article but it was recently brought up on the Standard Deviations podcast.\n\n\nHilary is giving out questionable career advice in the latest episode of @NSSDeviations.To launch the segment, we‚Äôre discussing: How do you know if a company has a good data science culture?https://t.co/SuRQDaQBlj\n\n‚Äî Roger D. Peng (@rdpeng) October 29, 2021\n\n\nExcluding people I have met at an R-user group, I know two people who understand file paths."
  },
  {
    "objectID": "posts/2023-02-18-goals/index.html",
    "href": "posts/2023-02-18-goals/index.html",
    "title": "Goals",
    "section": "",
    "text": "I am still lacking in direction but have settled to focus in on html/css. I can do some html but it is not second nature. I am going to be working through this course. I enjoy the structure of courses in that it helps you develop muscle memory.\nI wanted to play with Docker but every time I start docker desktop it freezes my computer. I need to set some boundaries with Digital Ocean and setting up Droplets and work on Shiny Proxy."
  },
  {
    "objectID": "posts/2023-02-19-one-down/index.html",
    "href": "posts/2023-02-19-one-down/index.html",
    "title": "One down",
    "section": "",
    "text": "News things I learned included the following html elements:\n\nfigure\nfigcaption\nfieldset\n\nAnchoring links and images is still a struggle and form elements is still a struggle. I look forward to the courses that focus on forms."
  },
  {
    "objectID": "posts/2022-12-29-today-i-learned/index.html",
    "href": "posts/2022-12-29-today-i-learned/index.html",
    "title": "Today I learned",
    "section": "",
    "text": "How have I not run into the problem of filtering multiple values. I was running through some scripts but could not find the solution."
  },
  {
    "objectID": "posts/2022-12-29-today-i-learned/index.html#solution",
    "href": "posts/2022-12-29-today-i-learned/index.html#solution",
    "title": "Today I learned",
    "section": "Solution:",
    "text": "Solution:\nI am pretty sure I found this on a stack overflow post\ndplyr::filter(stringr::str_detect(outcomes, 'something1|nextitem2|anotherthing3')) \nIt worked and write this to remember!"
  },
  {
    "objectID": "posts/2021-12-20-back-to-gitlab/index.html",
    "href": "posts/2021-12-20-back-to-gitlab/index.html",
    "title": "Back to Gitlab",
    "section": "",
    "text": "I started shifting to using Github mainly because of the usethis package\nI am thankful the folks at ThinkR have picked up the development of the gitlabr package. I really liked the ability to pull down and create issues directly from R."
  },
  {
    "objectID": "posts/sql/learn-sql.html",
    "href": "posts/sql/learn-sql.html",
    "title": "SQL Form",
    "section": "",
    "text": "ALTER TABLE <table_name> ADD COLUMN <column_name> <DATA_TYPE> <CONSTRAINTS>;"
  },
  {
    "objectID": "posts/sql/learn-sql.html#add-column-to-a-table",
    "href": "posts/sql/learn-sql.html#add-column-to-a-table",
    "title": "SQL Form",
    "section": "Add column to a table",
    "text": "Add column to a table\nALTER TABLE <table_name> ADD COLUMN <column_name> <DATA_TYPE> <CONSTRAINTS>;"
  },
  {
    "objectID": "posts/2021-12-29-submodules/index.html",
    "href": "posts/2021-12-29-submodules/index.html",
    "title": "Submodules",
    "section": "",
    "text": "I was trying to incorporate submodules into my Shiny Application. When I ran my application it was drawing blanks, but I don‚Äôt recall if there was an error message in the console.\nThis stack overflow post helped me figure it out. Nothing complicated‚Äì I just forgot the ns values in my submodules as well.\nHere is some additional resources on namespacing"
  },
  {
    "objectID": "posts/2022-11-10-playing/index.html",
    "href": "posts/2022-11-10-playing/index.html",
    "title": "playing",
    "section": "",
    "text": "Maybe I should try to write my own addin. I manged to get an addin in place but does not work. I need to feed the function arguments."
  },
  {
    "objectID": "posts/2021-10-04-domain-expertise/index.html",
    "href": "posts/2021-10-04-domain-expertise/index.html",
    "title": "Domain Expertise",
    "section": "",
    "text": "Conclusion\nDifferent but closely related."
  },
  {
    "objectID": "posts/2021-12-22-command-line/index.html",
    "href": "posts/2021-12-22-command-line/index.html",
    "title": "Command Line",
    "section": "",
    "text": "Here is a fun video/content on using the command line. Command Line. Good stuff with ls command and navigation."
  },
  {
    "objectID": "posts/2021-06-13-across/index.html",
    "href": "posts/2021-06-13-across/index.html",
    "title": "Across",
    "section": "",
    "text": "I was working on analyzing twenty plus survey questions and wanted to use the forcats function collapse on all of them. I was desperately trying to combine dplyr::across with the forcats function collapse.\nThankfully, I found this Stackoverflow Post\nlibrary(dplyr)\nlibrary(forcats)\ndf %>% \n    mutate(across(-pid, ~ fct_collapse(.,\n     yes = c('y', 'Y'), no = c('no', 'NO', 'n'))))\n     \nMy mistake was that I was trying to close the across function paren too soon and hence errors. Here is an example of one way I was doing it wrong.\nlibrary(dplyr)\nlibrary(forcats)\ndf %>% \n    mutate(across(-pid), ~ fct_collapse(.,\n     yes = c('y', 'Y'), no = c('no', 'NO', 'n'))))\n     \n\nConclusion\nI am hoping that digging into the reason for this trouble helps me remember for next time."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "R for Social Good",
    "section": "",
    "text": "Artilce on Pointblank Package\n\n\n\n\n\n\nApr 28, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSketching out ideas\n\n\n\n\n\n\nApr 5, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the grove back\n\n\n\n\n\n\nApr 3, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video gave me an idea\n\n\n\n\n\n\nFeb 21, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLove and Hate them\n\n\n\n\n\n\nFeb 20, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinished the Learn HTML Section\n\n\n\n\n\n\nFeb 19, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs and Outpus\n\n\n\n\n\n\nFeb 18, 2023\n\n\nKevin Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRember this post\n\n\n\n\n\n\nJan 15, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA good article on this topic\n\n\n\n\n\n\nJan 10, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPancakes\n\n\n\n\n\n\nJan 3, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking dates more difficult\n\n\n\n\n\n\nJan 2, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBegin again\n\n\n\n\n\n\nJan 1, 2023\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA script to filter multiple values\n\n\n\n\n\n\nDec 29, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen something is not working as expected; check the data type\n\n\n\n\n\n\nDec 16, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThanksgiving weekend\n\n\n\n\n\n\nNov 26, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Sheets\n\n\n\n\n\n\nNov 23, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodebooks\n\n\n\n\n\n\nNov 16, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe saga\n\n\n\n\n\n\nNov 15, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReprex and other items I learned today\n\n\n\n\n\n\nNov 12, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBe nice\n\n\n\n\n\n\nNov 11, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make good gadgets you have to make some stinkers\n\n\n\n\n\n\nNov 10, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with addins\n\n\n\n\n\n\nNov 10, 2022\n\n\nKevn Gilds, MPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkimbo\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2021\n\n\nkevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nevaluation\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2021\n\n\nkevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nKevin\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2019\n\n\nKevin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2018\n\n\nkevin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/freelance/index.html",
    "href": "posts/freelance/index.html",
    "title": "R for Social Good",
    "section": "",
    "text": "title: freelance-posts author: kevin date: ‚Äò2020-06-19‚Äô\n\nHow often do you formally analyze all the comments you receive. Below I will show you some ways to process text data for additional analysis.\nBelow is a simple analysis of my post from the Freelance Workshop. The workshop consisted of viewing a video by Seth Godin, and responding to question prompts and tagging . The key was writing your response and tagging other people to receive feedback and vice-versa.\nAt the end of the workshop, participants were able to download their posts into a csv file. I thought it may be good fun to analyze the data.\nI was surprised I only had 106 posts; it felt like much more and I am not sure had the capacity to write more.\n\n\n\n\nLoad Required packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(forcats)\nlibrary(viridis)\nlibrary(gt)\n\n\n\n\nBelow is a display of the first six rows and selected columns\n\npost %>%\n  dplyr::select(created_at, like_count, post) %>%\n  head() %>%\n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      created_at\n      like_count\n      post\n    \n  \n  \n    2020-03-30 20:56:58\n1\nFor kids: Adventures of Huckleberry Finn: Learn to see the world from many perspectives and learn empathy.\nFor adults: Lila An Inquiry Into Morals: Provides a framework for understanding life and change. \nFor people at work: The War of Art:  This book taught me how to work.\n    2020-03-31 01:21:54\n1\nThank you for the welcome @traviswilson . Good question, Finn and Lilla are about trying to understand people and society.\n    2020-03-31 20:37:27\n4\nHello! My name is Kevin Gilds residing in Lakeland, Florida--between Tampa and Orlando. I am here to learn how to take the next steps. I fell into my first freelancing gig and would love to do more.\n    2020-03-31 21:12:07\n2\nHi Razlan,\n\nI try to get the tough stuff done early while they are still asleep. We are fortunate that we have the flexibility to alternate care giving duties. The kids are also of the age where they are able to play together most of the time well enough. \n\nHang in there!\n    2020-03-31 23:53:03\n0\nThis is a great one; I let my former boss read and has not returned yet.... :upside_down_face:\n    2020-04-01 20:31:17\n3\nI am a huge fan of this podcast and blogging platform\n\nIt is from the creators of Basecamp.\n\nThe podcast:\nhttps://rework.fm/\n\nThe blogging/article platform\nhttps://m.signalvnoise.com/\n  \n  \n  \n\n\n\n\nI will use the tidytext package to break down the words in the post column and put it in a tidy format for analysis.\nLets take a peak\n\npost %>%\n  tidytext::unnest_tokens(word, post)%>%\n   dplyr::count(word) %>% # count number of words produces n\n   dplyr::arrange(desc(n)) %>%\n   dplyr::filter(n >=150) %>%\n    gt::gt() %>%\n      tab_header(\n    title = md(\"**Kevin's Top Words**\"),\n    subtitle = md(\"*The Freelance Workshop*\")\n  )\n\n\n\n\n\n  \n    \n      Kevin's Top Words\n    \n    \n      The Freelance Workshop\n    \n  \n  \n    \n      word\n      n\n    \n  \n  \n    to\n415\n    the\n344\n    i\n342\n    and\n237\n    a\n209\n    you\n202\n    is\n173\n    of\n161\n    this\n150\n  \n  \n  \n\n\n\n\nNot very interesting, but we can clean this up! The tidytext package has loaded stop words to help us remove common words, and I am going to add words specific to the workshop.\n\nComplete an anti-join to remove standard stop words and add your own custom words you wish to dismiss\n\nBelow are my custom words:\n\nword <- c(\"topic\",\"quote\", \"post\", \"1\", \"https\", \"hey\", \"2\", \"homeroom5\", \"5\", \"3\", \"6\", \"_kevin_\", \"_imposter_\", \"34974\", \"you‚Äôre\")\n\nworkshop_stop <- data.frame(word)\n\nBelow we have a script that adds some additional data processing to extract insights from the words. I added a case_when statement to deal with similiar words and the anti-join statement to remove non insightful words\n\nword_count <- post %>%\n  tidytext::unnest_tokens(word, post ) %>% #breakdown each word of the character vector\n  dplyr::mutate(word  = case_when( #make \n      word == \"client\" ~ \"clients\",\n      TRUE ~(word)\n    )) %>%  \n  dplyr::anti_join(stop_words) %>% #remove standard common words\n    dplyr::anti_join(workshop_stop) %>% #remove words specifc to the workshop that are not of valle\n  dplyr::count(word) %>% # count number of words produces n\n   dplyr::arrange(desc(n)) %>%\n   dplyr::filter(n >=16) %>%\n  gt::gt () %>%\n  tab_header(\n    title = md(\"**Kevin's Top Words**\"),\n    subtitle = md(\"*The Freelance Workshop*\")\n  )\n\nword_count\n\n\n\n\n\n  \n    \n      Kevin's Top Words\n    \n    \n      The Freelance Workshop\n    \n  \n  \n    \n      word\n      n\n    \n  \n  \n    data\n83\n    clients\n66\n    time\n29\n    evaluation\n26\n    people\n23\n    skills\n18\n    farid\n17\n    process\n16\n    story\n16\n  \n  \n  \n\n\n\n\nNow we are in position to graph the data. Below is a graph of my most frequent words with a mimimum of 16.\n\n\n\n\n\nCode for the graph is here:\nWhat were the most frequent post by category?\n\n\n\n\n\nCode for the graph is here:\nThis has been a basic introduction to text mining with R. It is possible to break down comments into their components. The most suprising word that stands out to me is time. It makes me want to re-read my post and understand the context better."
  }
]